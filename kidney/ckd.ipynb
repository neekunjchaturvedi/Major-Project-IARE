{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection and Classification of Chronic Kidney Disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chronic kidney disease (CKD), also known as chronic renal disease, is progressive loss in kidney function over a period of months or years [1]. If kidney disease gets worse, wastes can build to high levels in your blood and make you feel sick. Patient may develop complications like high blood pressure, anemia (low blood count), weak bones, poor nutritional health and nerve damage. Also, kidney disease increases your risk of having heart and blood vessel disease. These problems may happen slowly over a long period of time.[2]\n",
    "\n",
    "In this project,Chronic Kidney Disease dataset in UCI Machine learning repository [3] have been explored, which includes 24 attributes excluding the target label class and health parameters of 400 patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bp</th>\n",
       "      <th>sg</th>\n",
       "      <th>al</th>\n",
       "      <th>su</th>\n",
       "      <th>bgr</th>\n",
       "      <th>bu</th>\n",
       "      <th>sc</th>\n",
       "      <th>sod</th>\n",
       "      <th>pot</th>\n",
       "      <th>hemo</th>\n",
       "      <th>pcv</th>\n",
       "      <th>wbcc</th>\n",
       "      <th>rbcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>391.000000</td>\n",
       "      <td>388.000000</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>354.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>356.000000</td>\n",
       "      <td>381.000000</td>\n",
       "      <td>383.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>312.000000</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>329.000000</td>\n",
       "      <td>294.000000</td>\n",
       "      <td>269.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>51.483376</td>\n",
       "      <td>76.469072</td>\n",
       "      <td>1.017408</td>\n",
       "      <td>1.016949</td>\n",
       "      <td>0.450142</td>\n",
       "      <td>148.036517</td>\n",
       "      <td>57.425722</td>\n",
       "      <td>3.072454</td>\n",
       "      <td>137.528754</td>\n",
       "      <td>4.627244</td>\n",
       "      <td>12.526437</td>\n",
       "      <td>38.884498</td>\n",
       "      <td>8406.122449</td>\n",
       "      <td>4.707435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17.169714</td>\n",
       "      <td>13.683637</td>\n",
       "      <td>0.005717</td>\n",
       "      <td>1.352679</td>\n",
       "      <td>1.099191</td>\n",
       "      <td>79.281714</td>\n",
       "      <td>50.503006</td>\n",
       "      <td>5.741126</td>\n",
       "      <td>10.408752</td>\n",
       "      <td>3.193904</td>\n",
       "      <td>2.912587</td>\n",
       "      <td>8.990105</td>\n",
       "      <td>2944.474190</td>\n",
       "      <td>1.025323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.005000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>6500.000000</td>\n",
       "      <td>3.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>1.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>12.650000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>8000.000000</td>\n",
       "      <td>4.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>64.500000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>1.020000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>9800.000000</td>\n",
       "      <td>5.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>1.025000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>391.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>17.800000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>26400.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age          bp          sg          al          su         bgr  \\\n",
       "count  391.000000  388.000000  353.000000  354.000000  351.000000  356.000000   \n",
       "mean    51.483376   76.469072    1.017408    1.016949    0.450142  148.036517   \n",
       "std     17.169714   13.683637    0.005717    1.352679    1.099191   79.281714   \n",
       "min      2.000000   50.000000    1.005000    0.000000    0.000000   22.000000   \n",
       "25%     42.000000   70.000000    1.010000    0.000000    0.000000   99.000000   \n",
       "50%     55.000000   80.000000    1.020000    0.000000    0.000000  121.000000   \n",
       "75%     64.500000   80.000000    1.020000    2.000000    0.000000  163.000000   \n",
       "max     90.000000  180.000000    1.025000    5.000000    5.000000  490.000000   \n",
       "\n",
       "               bu          sc         sod         pot        hemo         pcv  \\\n",
       "count  381.000000  383.000000  313.000000  312.000000  348.000000  329.000000   \n",
       "mean    57.425722    3.072454  137.528754    4.627244   12.526437   38.884498   \n",
       "std     50.503006    5.741126   10.408752    3.193904    2.912587    8.990105   \n",
       "min      1.500000    0.400000    4.500000    2.500000    3.100000    9.000000   \n",
       "25%     27.000000    0.900000  135.000000    3.800000   10.300000   32.000000   \n",
       "50%     42.000000    1.300000  138.000000    4.400000   12.650000   40.000000   \n",
       "75%     66.000000    2.800000  142.000000    4.900000   15.000000   45.000000   \n",
       "max    391.000000   76.000000  163.000000   47.000000   17.800000   54.000000   \n",
       "\n",
       "               wbcc        rbcc  \n",
       "count    294.000000  269.000000  \n",
       "mean    8406.122449    4.707435  \n",
       "std     2944.474190    1.025323  \n",
       "min     2200.000000    2.100000  \n",
       "25%     6500.000000    3.900000  \n",
       "50%     8000.000000    4.800000  \n",
       "75%     9800.000000    5.400000  \n",
       "max    26400.000000    8.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from scipy.stats import mode\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('ckd.csv',sep=',', na_values=['?'])\n",
    "# print \"Attribute list is {}\".format(data.columns.values)\n",
    "\n",
    "print (\" \")\n",
    "#No of people with chronic kidney disease\n",
    "n_ckd = len(data[data['class']=='ckd'])\n",
    "\n",
    "#No of people without chronic kidney disease\n",
    "n_notckd = len(data[data['class']=='notckd'])\n",
    "\n",
    "\n",
    "\n",
    "# print \"Number of people detected with chronic kidney disease: {}\".format(n_ckd)\n",
    "# print \"Number of people not detected with chronic kidney diesease: {}\".format(n_notckd)\n",
    "display(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas.tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scatter_matrix\n\u001b[0;32m      2\u001b[0m scatter_matrix(data, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m12\u001b[39m), diagonal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkde\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas.tools'"
     ]
    }
   ],
   "source": [
    "from pandas.tools.plotting import scatter_matrix\n",
    "scatter_matrix(data, alpha=0.6, figsize=(12, 12), diagonal='kde')\n",
    "plt.show()\n",
    "\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(30,20))\n",
    "sns.heatmap(data.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 24 feature attributes with 11 numerical attributes and 14 nominal and one target label.The number of people detected with chronic kidney disease(ckd) is 250 and with no-ckd is 150. In the scatter plot, hemo,pcv and rbcc shows high positive correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling the Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      age    bp     sg   al   su     rbc        pc         pcc          ba  \\\n",
      "0    48.0  80.0  1.020  1.0  0.0  normal    normal  notpresent  notpresent   \n",
      "1     7.0  50.0  1.020  4.0  0.0  normal    normal  notpresent  notpresent   \n",
      "2    62.0  80.0  1.010  2.0  3.0  normal    normal  notpresent  notpresent   \n",
      "3    48.0  70.0  1.005  4.0  0.0  normal  abnormal     present  notpresent   \n",
      "4    51.0  80.0  1.010  2.0  0.0  normal    normal  notpresent  notpresent   \n",
      "..    ...   ...    ...  ...  ...     ...       ...         ...         ...   \n",
      "395  55.0  80.0  1.020  0.0  0.0  normal    normal  notpresent  notpresent   \n",
      "396  42.0  70.0  1.025  0.0  0.0  normal    normal  notpresent  notpresent   \n",
      "397  12.0  80.0  1.020  0.0  0.0  normal    normal  notpresent  notpresent   \n",
      "398  17.0  60.0  1.025  0.0  0.0  normal    normal  notpresent  notpresent   \n",
      "399  58.0  80.0  1.025  0.0  0.0  normal    normal  notpresent  notpresent   \n",
      "\n",
      "       bgr  ...   pcv    wbcc  rbcc  htn   dm  cad  appet   pe  ane   class  \n",
      "0    121.0  ...  44.0  7800.0   5.2  yes  yes   no   good   no   no     ckd  \n",
      "1    121.0  ...  38.0  6000.0   4.8   no   no   no   good   no   no     ckd  \n",
      "2    423.0  ...  31.0  7500.0   4.8   no  yes   no   poor   no  yes     ckd  \n",
      "3    117.0  ...  32.0  6700.0   3.9  yes   no   no   poor  yes  yes     ckd  \n",
      "4    106.0  ...  35.0  7300.0   4.6   no   no   no   good   no   no     ckd  \n",
      "..     ...  ...   ...     ...   ...  ...  ...  ...    ...  ...  ...     ...  \n",
      "395  140.0  ...  47.0  6700.0   4.9   no   no   no   good   no   no  notckd  \n",
      "396   75.0  ...  54.0  7800.0   6.2   no   no   no   good   no   no  notckd  \n",
      "397  100.0  ...  49.0  6600.0   5.4   no   no   no   good   no   no  notckd  \n",
      "398  114.0  ...  51.0  7200.0   5.9   no   no   no   good   no   no  notckd  \n",
      "399  131.0  ...  53.0  6800.0   6.1   no   no   no   good   no   no  notckd  \n",
      "\n",
      "[400 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "#Filling missing value with most frequent for nominal and median for numerical\n",
    "\n",
    "X = pd.DataFrame(data)\n",
    "fill = pd.Series([X[c].value_counts().index[0]\n",
    "        if X[c].dtype == np.dtype('O') else X[c].median() for c in X],\n",
    "        index=X.columns)\n",
    "new_data=X.fillna(fill) \n",
    "print (new_data )           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Encoding on the nominal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age      float64\n",
      "bp       float64\n",
      "sg       float64\n",
      "al       float64\n",
      "su       float64\n",
      "rbc        int64\n",
      "pc         int64\n",
      "pcc        int64\n",
      "ba         int64\n",
      "bgr      float64\n",
      "bu       float64\n",
      "sc       float64\n",
      "sod      float64\n",
      "pot      float64\n",
      "hemo     float64\n",
      "pcv      float64\n",
      "wbcc     float64\n",
      "rbcc     float64\n",
      "htn        int64\n",
      "dm         int64\n",
      "cad        int64\n",
      "appet      int64\n",
      "pe         int64\n",
      "ane        int64\n",
      "class      int64\n",
      "dtype: object\n",
      "      age     bp     sg   al   su  rbc  pc  pcc  ba    bgr  ...     pcv  \\\n",
      "0    48.0   80.0  1.020  1.0  0.0    1   1    0   0  121.0  ...    44.0   \n",
      "1     7.0   50.0  1.020  4.0  0.0    1   1    0   0  121.0  ...    38.0   \n",
      "2    62.0   80.0  1.010  2.0  3.0    1   1    0   0  423.0  ...    31.0   \n",
      "3    48.0   70.0  1.005  4.0  0.0    1   0    1   0  117.0  ...    32.0   \n",
      "4    51.0   80.0  1.010  2.0  0.0    1   1    0   0  106.0  ...    35.0   \n",
      "5    60.0   90.0  1.015  3.0  0.0    1   1    0   0   74.0  ...    39.0   \n",
      "6    68.0   70.0  1.010  0.0  0.0    1   1    0   0  100.0  ...    36.0   \n",
      "7    24.0   80.0  1.015  2.0  4.0    1   0    0   0  410.0  ...    44.0   \n",
      "8    52.0  100.0  1.015  3.0  0.0    1   0    1   0  138.0  ...    33.0   \n",
      "9    53.0   90.0  1.020  2.0  0.0    0   0    1   0   70.0  ...    29.0   \n",
      "10   50.0   60.0  1.010  2.0  4.0    1   0    1   0  490.0  ...    28.0   \n",
      "11   63.0   70.0  1.010  3.0  0.0    0   0    1   0  380.0  ...    32.0   \n",
      "12   68.0   70.0  1.015  3.0  1.0    1   1    1   0  208.0  ...    28.0   \n",
      "13   68.0   70.0  1.020  0.0  0.0    1   1    0   0   98.0  ...    40.0   \n",
      "14   68.0   80.0  1.010  3.0  2.0    1   0    1   1  157.0  ...    16.0   \n",
      "15   40.0   80.0  1.015  3.0  0.0    1   1    0   0   76.0  ...    24.0   \n",
      "16   47.0   70.0  1.015  2.0  0.0    1   1    0   0   99.0  ...    40.0   \n",
      "17   47.0   80.0  1.020  0.0  0.0    1   1    0   0  114.0  ...    40.0   \n",
      "18   60.0  100.0  1.025  0.0  3.0    1   1    0   0  263.0  ...    37.0   \n",
      "19   62.0   60.0  1.015  1.0  0.0    1   0    1   0  100.0  ...    30.0   \n",
      "20   61.0   80.0  1.015  2.0  0.0    0   0    0   0  173.0  ...    24.0   \n",
      "21   60.0   90.0  1.020  0.0  0.0    1   1    0   0  121.0  ...    32.0   \n",
      "22   48.0   80.0  1.025  4.0  0.0    1   0    0   0   95.0  ...    32.0   \n",
      "23   21.0   70.0  1.010  0.0  0.0    1   1    0   0  121.0  ...    40.0   \n",
      "24   42.0  100.0  1.015  4.0  0.0    1   0    0   1  121.0  ...    39.0   \n",
      "25   61.0   60.0  1.025  0.0  0.0    1   1    0   0  108.0  ...    29.0   \n",
      "26   75.0   80.0  1.015  0.0  0.0    1   1    0   0  156.0  ...    35.0   \n",
      "27   69.0   70.0  1.010  3.0  4.0    1   0    0   0  264.0  ...    37.0   \n",
      "28   75.0   70.0  1.020  1.0  3.0    1   1    0   0  123.0  ...    40.0   \n",
      "29   68.0   70.0  1.005  1.0  0.0    0   0    1   0  121.0  ...    38.0   \n",
      "..    ...    ...    ...  ...  ...  ...  ..  ...  ..    ...  ...     ...   \n",
      "370  69.0   70.0  1.020  0.0  0.0    1   1    0   0   83.0  ...    50.0   \n",
      "371  28.0   60.0  1.025  0.0  0.0    1   1    0   0   79.0  ...    51.0   \n",
      "372  72.0   60.0  1.020  0.0  0.0    1   1    0   0  109.0  ...    52.0   \n",
      "373  61.0   70.0  1.025  0.0  0.0    1   1    0   0  133.0  ...    47.0   \n",
      "374  79.0   80.0  1.025  0.0  0.0    1   1    0   0  111.0  ...    40.0   \n",
      "375  70.0   80.0  1.020  0.0  0.0    1   1    0   0   74.0  ...    48.0   \n",
      "376  58.0   70.0  1.025  0.0  0.0    1   1    0   0   88.0  ...    53.0   \n",
      "377  64.0   70.0  1.020  0.0  0.0    1   1    0   0   97.0  ...    49.0   \n",
      "378  71.0   60.0  1.025  0.0  0.0    1   1    0   0  121.0  ...    42.0   \n",
      "379  62.0   80.0  1.025  0.0  0.0    1   1    0   0   78.0  ...    50.0   \n",
      "380  59.0   60.0  1.020  0.0  0.0    1   1    0   0  113.0  ...    54.0   \n",
      "381  71.0   70.0  1.025  0.0  0.0    1   1    0   0   79.0  ...    40.0   \n",
      "382  48.0   80.0  1.025  0.0  0.0    1   1    0   0   75.0  ...    51.0   \n",
      "383  80.0   80.0  1.025  0.0  0.0    1   1    0   0  119.0  ...    49.0   \n",
      "384  57.0   60.0  1.020  0.0  0.0    1   1    0   0  132.0  ...    42.0   \n",
      "385  63.0   70.0  1.020  0.0  0.0    1   1    0   0  113.0  ...    52.0   \n",
      "386  46.0   70.0  1.025  0.0  0.0    1   1    0   0  100.0  ...    43.0   \n",
      "387  15.0   80.0  1.025  0.0  0.0    1   1    0   0   93.0  ...    50.0   \n",
      "388  51.0   80.0  1.020  0.0  0.0    1   1    0   0   94.0  ...    46.0   \n",
      "389  41.0   80.0  1.025  0.0  0.0    1   1    0   0  112.0  ...    52.0   \n",
      "390  52.0   80.0  1.025  0.0  0.0    1   1    0   0   99.0  ...    52.0   \n",
      "391  36.0   80.0  1.025  0.0  0.0    1   1    0   0   85.0  ...    44.0   \n",
      "392  57.0   80.0  1.020  0.0  0.0    1   1    0   0  133.0  ...    46.0   \n",
      "393  43.0   60.0  1.025  0.0  0.0    1   1    0   0  117.0  ...    54.0   \n",
      "394  50.0   80.0  1.020  0.0  0.0    1   1    0   0  137.0  ...    45.0   \n",
      "395  55.0   80.0  1.020  0.0  0.0    1   1    0   0  140.0  ...    47.0   \n",
      "396  42.0   70.0  1.025  0.0  0.0    1   1    0   0   75.0  ...    54.0   \n",
      "397  12.0   80.0  1.020  0.0  0.0    1   1    0   0  100.0  ...    49.0   \n",
      "398  17.0   60.0  1.025  0.0  0.0    1   1    0   0  114.0  ...    51.0   \n",
      "399  58.0   80.0  1.025  0.0  0.0    1   1    0   0  131.0  ...    53.0   \n",
      "\n",
      "        wbcc  rbcc  htn  dm  cad  appet  pe  ane  class  \n",
      "0     7800.0   5.2    1   1    0      0   0    0      0  \n",
      "1     6000.0   4.8    0   0    0      0   0    0      0  \n",
      "2     7500.0   4.8    0   1    0      1   0    1      0  \n",
      "3     6700.0   3.9    1   0    0      1   1    1      0  \n",
      "4     7300.0   4.6    0   0    0      0   0    0      0  \n",
      "5     7800.0   4.4    1   1    0      0   1    0      0  \n",
      "6     8000.0   4.8    0   0    0      0   0    0      0  \n",
      "7     6900.0   5.0    0   1    0      0   1    0      0  \n",
      "8     9600.0   4.0    1   1    0      0   0    1      0  \n",
      "9    12100.0   3.7    1   1    0      1   0    1      0  \n",
      "10    8000.0   4.8    1   1    0      0   0    1      0  \n",
      "11    4500.0   3.8    1   1    0      1   1    0      0  \n",
      "12   12200.0   3.4    1   1    1      1   1    0      0  \n",
      "13    8000.0   4.8    1   1    1      1   1    0      0  \n",
      "14   11000.0   2.6    1   1    1      1   1    0      0  \n",
      "15    3800.0   2.8    1   0    0      0   0    1      0  \n",
      "16    8000.0   4.8    0   0    0      0   0    0      0  \n",
      "17    8000.0   4.8    1   0    0      1   0    0      0  \n",
      "18   11400.0   4.3    1   1    1      0   0    0      0  \n",
      "19    5300.0   3.7    1   0    1      0   0    0      0  \n",
      "20    9200.0   3.2    1   1    1      1   1    1      0  \n",
      "21    6200.0   3.6    1   1    1      0   0    0      0  \n",
      "22    6900.0   3.4    1   0    0      0   0    1      0  \n",
      "23    8000.0   4.8    0   0    0      1   0    1      0  \n",
      "24    8300.0   4.6    1   0    0      1   0    0      0  \n",
      "25    8400.0   3.7    1   1    0      0   0    1      0  \n",
      "26   10300.0   4.0    1   1    0      1   0    0      0  \n",
      "27    9600.0   4.1    1   1    1      0   1    0      0  \n",
      "28    8000.0   4.8    0   1    0      0   0    0      0  \n",
      "29    8000.0   4.8    0   0    1      0   0    0      0  \n",
      "..       ...   ...  ...  ..  ...    ...  ..  ...    ...  \n",
      "370   9300.0   5.4    0   0    0      0   0    0      1  \n",
      "371   6500.0   5.0    0   0    0      0   0    0      1  \n",
      "372  10500.0   5.5    0   0    0      0   0    0      1  \n",
      "373   9200.0   4.9    0   0    0      0   0    0      1  \n",
      "374   8000.0   6.4    0   0    0      0   0    0      1  \n",
      "375   9700.0   5.6    0   0    0      0   0    0      1  \n",
      "376   9100.0   5.2    0   0    0      0   0    0      1  \n",
      "377   6400.0   4.8    0   0    0      0   0    0      1  \n",
      "378   7700.0   5.5    0   0    0      0   0    0      1  \n",
      "379   5400.0   5.7    0   0    0      0   0    0      1  \n",
      "380   6500.0   4.9    0   0    0      0   0    0      1  \n",
      "381   5800.0   5.9    0   0    0      0   0    0      1  \n",
      "382   6000.0   6.5    0   0    0      0   0    0      1  \n",
      "383   5100.0   5.0    0   0    0      0   0    0      1  \n",
      "384  11000.0   4.5    0   0    0      0   0    0      1  \n",
      "385   8000.0   5.1    0   0    0      0   0    0      1  \n",
      "386   5700.0   6.5    0   0    0      0   0    0      1  \n",
      "387   6200.0   5.2    0   0    0      0   0    0      1  \n",
      "388   9500.0   6.4    0   0    0      0   0    0      1  \n",
      "389   7200.0   5.8    0   0    0      0   0    0      1  \n",
      "390   6300.0   5.3    0   0    0      0   0    0      1  \n",
      "391   5800.0   6.3    0   0    0      0   0    0      1  \n",
      "392   6600.0   5.5    0   0    0      0   0    0      1  \n",
      "393   7400.0   5.4    0   0    0      0   0    0      1  \n",
      "394   9500.0   4.6    0   0    0      0   0    0      1  \n",
      "395   6700.0   4.9    0   0    0      0   0    0      1  \n",
      "396   7800.0   6.2    0   0    0      0   0    0      1  \n",
      "397   6600.0   5.4    0   0    0      0   0    0      1  \n",
      "398   7200.0   5.9    0   0    0      0   0    0      1  \n",
      "399   6800.0   6.1    0   0    0      0   0    0      1  \n",
      "\n",
      "[400 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "data=new_data.copy()\n",
    "for items in data:\n",
    "    if data[items].dtype == np.dtype('O'):\n",
    "        data[items]=le.fit_transform(data[items])\n",
    "\n",
    "print data.dtypes\n",
    "print data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling features using Robust Scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "target_class = data['class']\n",
    "features = data.drop('class', axis = 1)\n",
    "data_robust = pd.DataFrame(RobustScaler().fit_transform(features), columns=features.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 24)\n",
      "[ 0.38157857  0.25203875  0.10337358  0.05525873  0.04128847  0.0322456\n",
      "  0.02664966  0.02054982  0.01970661  0.01407955  0.01267326  0.00877071\n",
      "  0.00745055  0.00447937  0.00329693  0.00259371  0.00241495  0.00227722\n",
      "  0.00216587  0.00189897  0.00181543  0.00136625  0.0010538   0.00097364]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "data_pca = pd.DataFrame(pca.fit_transform(data_robust), columns=data_robust.columns)\n",
    "print data_robust.shape\n",
    "print pca.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 11 components gives principal components that explain at least 95% of total variance. So let the number of components be 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       dim1      dim2      dim3      dim4      dim5      dim6      dim7  \\\n",
      "0 -1.127701  0.779780  0.642923  0.190267  0.609335  0.444339  0.020288   \n",
      "1 -1.408005  1.026680  1.063298 -0.043320 -2.451236  0.824565 -0.597719   \n",
      "2  0.248468 -0.277980 -5.114364  3.851732 -0.211125 -0.270017 -0.949867   \n",
      "3 -0.491470 -3.934303 -1.223619 -1.503578 -1.921622  3.695031  0.158452   \n",
      "4 -0.775043  0.395504 -0.290283 -0.980759  0.116809  0.880136  0.586873   \n",
      "\n",
      "       dim8      dim9     dim10     dim11  \n",
      "0 -0.280839 -0.007856 -0.222121 -0.000352  \n",
      "1  0.786148  0.299144  1.906853  0.017050  \n",
      "2  0.489734  0.624682  0.248006 -0.615226  \n",
      "3 -0.023544  1.851161 -0.102150  0.294735  \n",
      "4  1.217560  1.012095 -0.188106 -0.184063  \n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=11)\n",
    "pca.fit(data_robust)\n",
    "reduced_data = pca.transform(data_robust)\n",
    "reduced_data = pd.DataFrame(reduced_data, columns = ['dim1','dim2','dim3','dim4','dim5','dim6','dim7','dim8','dim9','dim10','dim11'])\n",
    "print reduced_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset to training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SREE\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(reduced_data,target_class, test_size=0.25, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Six Classifiers without hypertuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SREE\\Anaconda2\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GaussianNB: \n",
      "\n",
      "Trained model in 2.0001 milliseconds\n",
      "\n",
      "DecisionTreeClassifier: \n",
      "\n",
      "Trained model in 1.9999 milliseconds\n",
      "\n",
      "SVC: \n",
      "\n",
      "Trained model in 2.0001 milliseconds\n",
      "\n",
      "RandomForestClassifier: \n",
      "\n",
      "Trained model in 27.9999 milliseconds\n",
      "\n",
      "KNeighborsClassifier: \n",
      "\n",
      "Trained model in 0.0000 milliseconds\n",
      "\n",
      "MLPClassifier: \n",
      "\n",
      "Trained model in 374.0001 milliseconds\n",
      "\n",
      "GaussianNB: \n",
      "\n",
      "f1 score for test test is 0.96\n",
      "\n",
      "DecisionTreeClassifier: \n",
      "\n",
      "f1 score for test test is 0.954545454545\n",
      "\n",
      "SVC: \n",
      "\n",
      "f1 score for test test is 0.984375\n",
      "\n",
      "RandomForestClassifier: \n",
      "\n",
      "f1 score for test test is 0.969696969697\n",
      "\n",
      "KNeighborsClassifier: \n",
      "\n",
      "f1 score for test test is 0.992248062016\n",
      "\n",
      "MLPClassifier: \n",
      "\n",
      "f1 score for test test is 0.992248062016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SREE\\Anaconda2\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from time import time\n",
    "clf_A=GaussianNB()\n",
    "clf_B=tree.DecisionTreeClassifier(random_state=0)\n",
    "clf_C=SVC()\n",
    "clf_D=RandomForestClassifier()\n",
    "clf_E=KNeighborsClassifier()\n",
    "clf_F=MLPClassifier(random_state=1)\n",
    "\n",
    "for clf in [clf_A, clf_B, clf_C, clf_D, clf_E, clf_F]:\n",
    "    \n",
    "    print \"\\n{}: \\n\".format(clf.__class__.__name__)\n",
    "    start = time()\n",
    "  \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    end = time()\n",
    "    print \"Trained model in {:.4f} milliseconds\".format((end - start)*1000)\n",
    "    \n",
    "for clf in [clf_A, clf_B, clf_C, clf_D, clf_E, clf_F]:\n",
    "    y_pred = clf.predict(X_test)\n",
    "    f1_scorer=f1_score(y_test, y_pred, pos_label=0)\n",
    "    print \"\\n{}: \\n\".format(clf.__class__.__name__)\n",
    "    print \"f1 score for test test is {}\".format(f1_scorer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Classifier        | Training Time (ms)         | F1 Score  |\n",
    "| ------------------|:--------------------------:| ---------:|\n",
    "| Gaussian NB       | 0.0000                     | 0.96      |\n",
    "| Decision Tree     | 0.0000                     | 0.9545    |\n",
    "| SVC               | 15.0001                    | 0.9843    |\n",
    "| Random Forest     | 46.9999                    | 0.9767    |\n",
    "| KNN               | 0.000                      | 0.9922    |\n",
    "| MLP               | 375.9999                   | 0.9922    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifiers with hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for test set: 0.984375\n",
      "Confusion Matrix is : \n",
      "  [[63  2]\n",
      " [ 0 35]] \n",
      " \n",
      "Classification report is : \n",
      "  \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.97      0.98        65\n",
      "    class 1       0.95      1.00      0.97        35\n",
      "\n",
      "avg / total       0.98      0.98      0.98       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "parameters = [{'kernel':['rbf','sigmoid'],'C': [.1,1,10], 'gamma': [0.001,10,1000]},]\n",
    "clf = SVC()\n",
    "f1_scorer = make_scorer(f1_score,pos_label=0)\n",
    "sss = StratifiedShuffleSplit( y_train, n_iter=10, test_size=0.25)\n",
    "grid_obj = GridSearchCV(clf,parameters,cv = sss,scoring=f1_scorer)\n",
    "grid_obj = grid_obj.fit(X_train,y_train)\n",
    "clf = grid_obj.best_estimator_\n",
    "#print clf\n",
    "y_pred = clf.predict(X_test)\n",
    "f1_score_value=f1_score(y_test, y_pred, pos_label=0) # For testing\n",
    "print \"F1 Score for test set: {}\".format(f1_score_value)\n",
    "print \"Confusion Matrix is : \\n  {} \".format(confusion_matrix(y_test, y_pred))\n",
    "target_names = ['class 0', 'class 1']\n",
    "print \" \"\n",
    "print \"Classification report is : \\n  \"\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for test set: 0.969230769231\n",
      "Confusion Matrix is : \n",
      "  [[63  2]\n",
      " [ 2 33]] \n",
      " \n",
      "Classification report is : \n",
      "  \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.97      0.97      0.97        65\n",
      "    class 1       0.94      0.94      0.94        35\n",
      "\n",
      "avg / total       0.96      0.96      0.96       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "parameters = [{'n_estimators': [10,50,75],\n",
    "               'max_features': [\"auto\",\"sqrt\",\"log2\"],'n_jobs': [1]}]\n",
    "clf = RandomForestClassifier()\n",
    "f1_scorer = make_scorer(f1_score,pos_label=0)\n",
    "sss = StratifiedShuffleSplit( y_train, n_iter=10, test_size=0.25)\n",
    "grid_obj = GridSearchCV(clf,parameters,cv = sss,scoring=f1_scorer)\n",
    "grid_obj = grid_obj.fit(X_train,y_train)\n",
    "clf = grid_obj.best_estimator_\n",
    "y_pred = clf.predict(X_test)\n",
    "f1_score_value=f1_score(y_test, y_pred, pos_label=0) # For testing\n",
    "print \"F1 Score for test set: {}\".format(f1_score_value)\n",
    "print \"Confusion Matrix is : \\n  {} \".format(confusion_matrix(y_test, y_pred))\n",
    "target_names = ['class 0', 'class 1']\n",
    "print \" \"\n",
    "print \"Classification report is : \\n  \"\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "F1 Score for test set: 0.992248062016\n",
      "Confusion Matrix is : \n",
      "  [[64  1]\n",
      " [ 0 35]] \n",
      " \n",
      "Classification report is : \n",
      "  \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.98      0.99        65\n",
      "    class 1       0.97      1.00      0.99        35\n",
      "\n",
      "avg / total       0.99      0.99      0.99       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "parameters = [{'metric':['minkowski','euclidean'] ,\n",
    "               'weights':['uniform'],'n_neighbors': [5,15,10]}]\n",
    "clf = KNeighborsClassifier()\n",
    "f1_scorer = make_scorer(f1_score,pos_label=0)\n",
    "sss = StratifiedShuffleSplit( y_train, n_iter=10, test_size=0.25)\n",
    "grid_obj = GridSearchCV(clf,parameters,cv = sss,scoring=f1_scorer)\n",
    "grid_obj = grid_obj.fit(X_train,y_train)\n",
    "clf = grid_obj.best_estimator_\n",
    "print clf\n",
    "y_pred = clf.predict(X_test)\n",
    "f1_score_value=f1_score(y_test, y_pred, pos_label=0) # For testing\n",
    "print \"F1 Score for test set: {}\".format(f1_score_value)\n",
    "print \"Confusion Matrix is : \\n  {} \".format(confusion_matrix(y_test, y_pred))\n",
    "target_names = ['class 0', 'class 1']\n",
    "print \" \"\n",
    "print \"Classification report is : \\n  \"\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for test set: 0.992248062016\n",
      "Confusion Matrix is : \n",
      "  [[64  1]\n",
      " [ 0 35]] \n",
      " \n",
      "Classification report is : \n",
      "  \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.98      0.99        65\n",
      "    class 1       0.97      1.00      0.99        35\n",
      "\n",
      "avg / total       0.99      0.99      0.99       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report\n",
    "parameters = [{'criterion':['entropy','gini'] ,'splitter':['best','random']}]\n",
    "clf = tree.DecisionTreeClassifier(random_state=0)\n",
    "f1_scorer = make_scorer(f1_score,pos_label=0)\n",
    "sss = StratifiedShuffleSplit( y_train, n_iter=10, test_size=0.25)\n",
    "grid_obj = GridSearchCV(clf,parameters,cv = sss,scoring=f1_scorer)\n",
    "grid_obj = grid_obj.fit(X_train,y_train)\n",
    "clf = grid_obj.best_estimator_\n",
    "y_pred = clf.predict(X_test)\n",
    "f1_score_value=f1_score(y_test, y_pred, pos_label=0) # For testing\n",
    "print \"F1 Score for test set: {}\".format(f1_score_value)\n",
    "print \"Confusion Matrix is : \\n  {} \".format(confusion_matrix(y_test, y_pred))\n",
    "target_names = ['class 0', 'class 1']\n",
    "print \" \"\n",
    "print \"Classification report is : \\n  \"\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for test set: 0.992248062016\n",
      "Confusion Matrix is : \n",
      "  [[64  1]\n",
      " [ 0 35]] \n",
      " \n",
      "Classification report is : \n",
      "  \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.98      0.99        65\n",
      "    class 1       0.97      1.00      0.99        35\n",
      "\n",
      "avg / total       0.99      0.99      0.99       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "parameters = [{'solver':['lbfgs','sgd'],'alpha':[1e-6,1e-3],'hidden_layer_sizes':[3,6], }]\n",
    "clf = MLPClassifier(random_state=42)\n",
    "f1_scorer = make_scorer(f1_score,pos_label=0)\n",
    "sss = StratifiedShuffleSplit( y_train, n_iter=10, test_size=0.25)\n",
    "grid_obj = GridSearchCV(clf,parameters,cv = sss,scoring=f1_scorer)\n",
    "grid_obj = grid_obj.fit(X_train,y_train)\n",
    "clf = grid_obj.best_estimator_\n",
    "y_pred = clf.predict(X_test)\n",
    "f1_score_value=f1_score(y_test, y_pred, pos_label=0) # For testing\n",
    "print \"F1 Score for test set: {}\".format(f1_score_value)\n",
    "print \"Confusion Matrix is : \\n  {} \".format(confusion_matrix(y_test, y_pred))\n",
    "target_names = ['class 0', 'class 1']\n",
    "print \" \"\n",
    "print \"Classification report is : \\n  \"\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for test set: 0.96\n",
      "Confusion Matrix is : \n",
      "  [[60  5]\n",
      " [ 0 35]] \n",
      " \n",
      "Classification report is : \n",
      "  \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.92      0.96        65\n",
      "    class 1       0.88      1.00      0.93        35\n",
      "\n",
      "avg / total       0.96      0.95      0.95       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train,y_train).predict(X_test)\n",
    "\n",
    "f1_score_value=f1_score(y_test, y_pred, pos_label=0) # For testing\n",
    "print \"F1 Score for test set: {}\".format(f1_score_value)\n",
    "print \"Confusion Matrix is : \\n  {} \".format(confusion_matrix(y_test, y_pred))\n",
    "target_names = ['class 0', 'class 1']\n",
    "print \" \"\n",
    "print \"Classification report is : \\n  \"\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With Naive Bayes(NB) as a benchmark of evaluation with F1 score of NB being 0.96, KNN outperforms NB with 0.99 with and without hypertuning and completing the training in less time.ANN also outperforms NB in terms of F1 score but it takes much training time even without hypertuning compared to others.Decision Tree shows an increase in performance with parameter optimization.\n",
    "So comparing to the benchmark NB, all classifiers performs well but KNN performs really well with better accuracy and less time as medical diagnostic systems must give high performance and accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "    \n",
    "    [1] https://en.wikipedia.org/wiki/Chronic_kidney_disease\n",
    "    [2] https://www.kidney.org/kidneydisease/aboutckd\n",
    "    [3]  L. Rubini, “Early stage of chronic kidney disease UCI machine learning repository,” 2015. [Online]. Available:           http://archive.ics.uci.edu/ml/datasets/Chronic Kidney Disease \n",
    "    [4] http://www.worldkidneyday.org/faqs/chronic-kidney-disease/\n",
    "    [5] http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "    [6] K.M. Al-Aidaroos, A.A. Bakar and Z. Othman ,Medical Data Classification with Naive Bayes Approach\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
